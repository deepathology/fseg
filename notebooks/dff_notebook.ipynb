{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fseg.fseg import FSeg, show_segmentation_on_image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam.utils.image import preprocess_image\n",
    "from torchvision.models import resnet50\n",
    "import torch\n",
    "import cv2\n",
    "import timm\n",
    "import math\n",
    "import joblib\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Login\n",
    "\n",
    "In the next cell, we will log in to Hugging Face using an authentication token. This step is necessary to access and download models from the Hugging Face Hub, which we'll be using in our notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"<Put your token here>\"\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Reshape Transform\n",
    "\n",
    "For using a transformer-based model, we need to define a `TransformerReshapeTransform` class as shown in the next cell. This class is essential for reshaping the output tensor from the transformer model to make it compatible with the segmentation task.\n",
    "\n",
    "The `TransformerReshapeTransform` class performs the following operations:\n",
    "1. Reshapes the input tensor\n",
    "2. Applies a ReLU activation\n",
    "3. Rearranges the dimensions to match the expected format for segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerReshapeTransform:\n",
    "    def __init__(self):\n",
    "        self.input_tensor_shape = None\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        result = torch.nn.ReLU()(tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "                                self.input_tensor_shape[2] // 16,\n",
    "                                self.input_tensor_shape[3] // 16,\n",
    "                                tensor.size(2)))\n",
    "        # Bring the channels to the first dimension,\n",
    "        # like in CNNs.\n",
    "        result = result.transpose(2, 3).transpose(1, 2)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing the Example Image\n",
    "\n",
    "It's important to note that the UNI and Prov-GigaPath models require input images with dimensions divisible by 16.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_divisible(img: np.ndarray, number: int = 16) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pad the image shape to be a multiple of number\n",
    "\n",
    "    :param img: Input image.\n",
    "    :param scale: Scale factor.\n",
    "    :return: Padded input image.\n",
    "    \"\"\"\n",
    "    w = number * math.ceil(img.shape[1] / number) - img.shape[1]\n",
    "    h = number * math.ceil(img.shape[0] / number) - img.shape[0]\n",
    "\n",
    "    if w > 0 or h > 0:\n",
    "        padded = cv2.copyMakeBorder(\n",
    "            img, 0, h, 0, w, borderType=cv2.BORDER_REPLICATE)\n",
    "        return padded\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "\n",
    "img_path = \"../images/example_3.jpg\"\n",
    "img = np.array(Image.open(img_path))[:, :, :3]\n",
    "orig_shape = img.shape\n",
    "img = pad_divisible(img)\n",
    "rgb_img_float = np.float32(img) / 255\n",
    "input_tensor = preprocess_image(rgb_img_float,\n",
    "                                mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Model and Getting Results on the Image\n",
    "\n",
    "We'll load a pre-trained model and use it to generate segmentation results on our input image using the concepts (i.e. model embeddings) as the projection matrix. We'll follow these steps:\n",
    "\n",
    "1. Choose and load a pre-trained model (UNI, GigaPath, or ResNet50)\n",
    "2. Set up the target layer and any necessary transformations\n",
    "3. Initialize the DFFSeg object\n",
    "4. Load pre-computed model embeddings\n",
    "5. Define the number of clusters and prepare the concepts\n",
    "6. Generate the segmentation prediction\n",
    "\n",
    "The code below demonstrates this process, allowing us to see how the chosen model performs on our example image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"uni\"\n",
    "\n",
    "if model_name == \"gigapath\":\n",
    "    model = timm.create_model(\"hf_hub:prov-gigapath/prov-gigapath\", pretrained=True, dynamic_img_size=True)\n",
    "    target_layer = model.blocks[-1]\n",
    "    transform = TransformerReshapeTransform()\n",
    "if model_name == \"resnet50\":\n",
    "    concepts = np.load(\"/home/gildenbj/dev/dff_seg/concepts_resnet50_concepts.npy\")\n",
    "    model = resnet50(pretrained=True)\n",
    "    target_layer = model.layer3\n",
    "    transform = None\n",
    "elif model_name == \"uni\":\n",
    "    model = timm.create_model(\"hf-hub:MahmoodLab/uni\", pretrained=True, init_values=1e-5, dynamic_img_size=True)\n",
    "    target_layer = model.blocks[-1]    \n",
    "    transform = TransformerReshapeTransform()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "unsupervised_seg = FSeg(\n",
    "    model=model,\n",
    "    target_layer=target_layer,\n",
    "    reshape_transform=transform,\n",
    ")\n",
    "\n",
    "# Load the clustering models embeddings\n",
    "model_embeddings = joblib.load(\"../model_embeddings/uni.joblib\")\n",
    "\n",
    "k = 16  # Number of clusters\n",
    "unsupervised_seg.reshape_transform.input_tensor_shape = input_tensor.shape\n",
    "\n",
    "# Define the concepts as the k-means centroids and make sure they are non-negative\n",
    "concepts = model_embeddings[k]\n",
    "concepts[concepts < 0] = 0\n",
    "\n",
    "segmentation = unsupervised_seg.predict_project_concepts(input_tensor, concepts)\n",
    "visualization = show_segmentation_on_image(\n",
    "    img=img,\n",
    "    segmentation=segmentation,\n",
    "    image_weight=0.7,\n",
    "    n_categories=len(concepts))\n",
    "\n",
    "# Unpad the result segmentation to match the original image dimensions\n",
    "visualization = visualization[:orig_shape[0], :orig_shape[1], :]\n",
    "\n",
    "display(Image.fromarray(visualization))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Concepts Similarity for Segmentation\n",
    "\n",
    "In the next cell, we will use the concepts similarity approach to generate the segmentation results. This method compares the similarity between the extracted features and predefined concepts to create a segmentation map. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "class ConceptClustering:\n",
    "    def __init__(self, clusters: np.ndarray):\n",
    "        \"\"\"\n",
    "        Clustering model based on the cosine similarity between the concept\n",
    "        embeddings.\n",
    "\n",
    "        :param clusters: The clusters centroids embeddings.\n",
    "\n",
    "        \"\"\"\n",
    "        self.clusters = clusters\n",
    "\n",
    "    def __call__(self, vector: np.ndarray) -> int:\n",
    "        return cosine_distances(vector, self.clusters).argmin()\n",
    "\n",
    "clustering_model = ConceptClustering(concepts)\n",
    "segmentation = unsupervised_seg.predict_clustering(input_tensor, clustering_model)\n",
    "visualization = show_segmentation_on_image(\n",
    "    img=img,\n",
    "    segmentation=segmentation,\n",
    "    image_weight=0.7,\n",
    "    n_categories=len(concepts))\n",
    "\n",
    "# Unpad the result segmentation to match the original image dimensions\n",
    "visualization = visualization[:orig_shape[0], :orig_shape[1], :]\n",
    "\n",
    "display(Image.fromarray(visualization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptClustering:\n",
    "    def __init__(self, clusters: np.ndarray):\n",
    "        \"\"\"\n",
    "        Clustering model based on the cosine similarity between the concept\n",
    "        embeddings.\n",
    "\n",
    "        :param clusters: The clusters centroids embeddings.\n",
    "\n",
    "        \"\"\"\n",
    "        self.clusters = clusters\n",
    "\n",
    "    def __call__(self, vectors: np.ndarray) -> int:\n",
    "        return cosine_distances(vectors, self.clusters).argmin(axis=-1)\n",
    "\n",
    "clustering_model = ConceptClustering(concepts)\n",
    "segmentation = unsupervised_seg.predict_clustering_without_factorization(input_tensor, clustering_model)\n",
    "visualization = show_segmentation_on_image(\n",
    "    img=img,\n",
    "    segmentation=segmentation,\n",
    "    image_weight=0.7,\n",
    "    n_categories=len(concepts))\n",
    "\n",
    "# Unpad the result segmentation to match the original image dimensions\n",
    "visualization = visualization[:orig_shape[0], :orig_shape[1], :]\n",
    "\n",
    "display(Image.fromarray(visualization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
