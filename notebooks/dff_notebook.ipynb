{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/gildenbj/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "token = \"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\"\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1024)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "cluster_model = joblib.load(\"/gstore/data/dp_labs/tcga/1000_per_slide/embeddings/_uni/5239_m_10_0_tile_256_step_256_mask_atd_image_format_png_NONE_embed_imagenet_backbone_backbone_uni_model_name_uni_aug_0_stain_None/aggregated/clusters_model_128.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1024)\n",
      "[[0.17423524 0.38282013 0.16792561 ... 0.23269433 0.3017246  0.12458277]\n",
      " [0.18642387 0.28968507 0.16978127 ... 0.25196546 0.26143107 0.11726301]\n",
      " [0.16904913 0.28885865 0.15911302 ... 0.18029368 0.22724226 0.10550998]\n",
      " ...\n",
      " [0.18573405 0.13777147 0.1582502  ... 0.16789979 0.24432997 0.09890669]\n",
      " [0.03561661 0.2147832  0.05925214 ... 0.09241623 0.11835353 0.02232988]\n",
      " [0.15493439 0.18456897 0.17777306 ... 0.17943795 0.2152016  0.1111009 ]]\n",
      "-5.3326232e-12 4.3461933\n",
      "torch.Size([1, 3, 1024, 1440])\n",
      "(1, 64, 90, 1024) (5760, 1024) (32, 1024)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to NMF (input H)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 107\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mprint\u001b[39m(concepts)\n\u001b[1;32m    106\u001b[0m \u001b[39mprint\u001b[39m(concepts\u001b[39m.\u001b[39mmin(), concepts\u001b[39m.\u001b[39mmax())\n\u001b[0;32m--> 107\u001b[0m segmentation \u001b[39m=\u001b[39m unsupervised_seg\u001b[39m.\u001b[39;49mpredict_project_concepts(input_tensor, concepts)\n\u001b[1;32m    108\u001b[0m visualization \u001b[39m=\u001b[39m show_segmentation_on_image(\n\u001b[1;32m    109\u001b[0m     img\u001b[39m=\u001b[39mimg,\n\u001b[1;32m    110\u001b[0m     segmentation\u001b[39m=\u001b[39msegmentation,\n\u001b[1;32m    111\u001b[0m     image_weight\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m,\n\u001b[1;32m    112\u001b[0m     n_categories\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(concepts))\n\u001b[1;32m    114\u001b[0m unsupervised_seg\u001b[39m.\u001b[39mreshape_transform\u001b[39m.\u001b[39minput_tensor_shape \u001b[39m=\u001b[39m input_tensor2\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/dev/dff_seg/dff_seg/dff_seg.py:256\u001b[0m, in \u001b[0;36mDFFSeg.predict_project_concepts\u001b[0;34m(self, input_tensor, concepts)\u001b[0m\n\u001b[1;32m    254\u001b[0m vector \u001b[39m=\u001b[39m activations\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, activations\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    255\u001b[0m \u001b[39mprint\u001b[39m(activations\u001b[39m.\u001b[39mshape, vector\u001b[39m.\u001b[39mshape, concepts\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 256\u001b[0m w, __, __ \u001b[39m=\u001b[39m non_negative_factorization(\n\u001b[1;32m    257\u001b[0m     X\u001b[39m=\u001b[39;49mvector,\n\u001b[1;32m    258\u001b[0m     H\u001b[39m=\u001b[39;49mconcepts,\n\u001b[1;32m    259\u001b[0m     W\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    260\u001b[0m     n_components\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(concepts),\n\u001b[1;32m    261\u001b[0m     update_H\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    262\u001b[0m     random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[1;32m    263\u001b[0m     max_iter\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m,\n\u001b[1;32m    264\u001b[0m )\n\u001b[1;32m    266\u001b[0m w \u001b[39m=\u001b[39m w\u001b[39m.\u001b[39mreshape((activations\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], activations\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    268\u001b[0m w_for_resize \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m    269\u001b[0m     w\u001b[39m.\u001b[39mtranspose(\n\u001b[1;32m    270\u001b[0m         (\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))[\n\u001b[1;32m    271\u001b[0m         \u001b[39mNone\u001b[39;00m, :, :, :])  \u001b[39m# Add batch dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1153\u001b[0m, in \u001b[0;36mnon_negative_factorization\u001b[0;34m(X, W, H, n_components, init, update_H, solver, beta_loss, tol, max_iter, alpha, alpha_W, alpha_H, l1_ratio, regularization, random_state, verbose, shuffle)\u001b[0m\n\u001b[1;32m   1135\u001b[0m est \u001b[39m=\u001b[39m NMF(\n\u001b[1;32m   1136\u001b[0m     n_components\u001b[39m=\u001b[39mn_components,\n\u001b[1;32m   1137\u001b[0m     init\u001b[39m=\u001b[39minit,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     regularization\u001b[39m=\u001b[39mregularization,\n\u001b[1;32m   1150\u001b[0m )\n\u001b[1;32m   1152\u001b[0m \u001b[39mwith\u001b[39;00m config_context(assume_finite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m-> 1153\u001b[0m     W, H, n_iter \u001b[39m=\u001b[39m est\u001b[39m.\u001b[39;49m_fit_transform(X, W\u001b[39m=\u001b[39;49mW, H\u001b[39m=\u001b[39;49mH, update_H\u001b[39m=\u001b[39;49mupdate_H)\n\u001b[1;32m   1155\u001b[0m \u001b[39mreturn\u001b[39;00m W, H, n_iter\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1652\u001b[0m, in \u001b[0;36mNMF._fit_transform\u001b[0;34m(self, X, y, W, H, update_H)\u001b[0m\n\u001b[1;32m   1645\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen beta_loss <= 0 and X contains zeros, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1647\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe solver may diverge. Please add small values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1648\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto X, or use a positive beta_loss.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1649\u001b[0m     )\n\u001b[1;32m   1651\u001b[0m \u001b[39m# initialize or check W and H\u001b[39;00m\n\u001b[0;32m-> 1652\u001b[0m W, H \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_w_h(X, W, H, update_H)\n\u001b[1;32m   1654\u001b[0m \u001b[39m# scale the regularization terms\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scale_regularization(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1525\u001b[0m, in \u001b[0;36mNMF._check_w_h\u001b[0;34m(self, X, W, H, update_H)\u001b[0m\n\u001b[1;32m   1520\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   1521\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mH and W should have the same dtype as X. Got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1522\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mH.dtype = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and W.dtype = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(H\u001b[39m.\u001b[39mdtype, W\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m   1523\u001b[0m         )\n\u001b[1;32m   1524\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m update_H:\n\u001b[0;32m-> 1525\u001b[0m     _check_init(H, (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_n_components, n_features), \u001b[39m\"\u001b[39;49m\u001b[39mNMF (input H)\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1526\u001b[0m     \u001b[39mif\u001b[39;00m H\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m X\u001b[39m.\u001b[39mdtype:\n\u001b[1;32m   1527\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   1528\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mH should have the same dtype as X. Got H.dtype = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1529\u001b[0m                 H\u001b[39m.\u001b[39mdtype\n\u001b[1;32m   1530\u001b[0m             )\n\u001b[1;32m   1531\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:65\u001b[0m, in \u001b[0;36m_check_init\u001b[0;34m(A, shape, whom)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mshape(A) \u001b[39m!=\u001b[39m shape:\n\u001b[1;32m     61\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     62\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArray with wrong shape passed to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Expected \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[39m%\u001b[39m (whom, shape, np\u001b[39m.\u001b[39mshape(A))\n\u001b[1;32m     64\u001b[0m     )\n\u001b[0;32m---> 65\u001b[0m check_non_negative(A, whom)\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmax(A) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     67\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mArray passed to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is full of zeros.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m whom)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/sklearn/utils/validation.py:1372\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     X_min \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mmin()\n\u001b[1;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m X_min \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1372\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNegative values in data passed to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m whom)\n",
      "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to NMF (input H)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import dff_seg.dff_seg\n",
    "importlib.reload(dff_seg.dff_seg)\n",
    "from dff_seg.dff_seg import DFFSeg, show_segmentation_on_image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam.utils.image import preprocess_image\n",
    "from torchvision.models import resnet50\n",
    "from functools import partial\n",
    "import torch\n",
    "import cv2\n",
    "import timm\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "cluster_model = joblib.load(\"/gstore/data/dp_labs/tcga/1000_per_slide/embeddings/_uni/5239_m_10_0_tile_256_step_256_mask_atd_image_format_png_NONE_embed_imagenet_backbone_backbone_uni_model_name_uni_aug_0_stain_None/aggregated/clusters_model_32.joblib\")\n",
    "\n",
    "def pad_divisible(img: np.ndarray, number: int = 16) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pad the image shape to be a multiple of number\n",
    "\n",
    "    :param img: Input image.\n",
    "    :param scale: Scale factor.\n",
    "    :return: Padded input image.\n",
    "    \"\"\"\n",
    "    w = number * math.ceil(img.shape[1] / number) - img.shape[1]\n",
    "    h = number * math.ceil(img.shape[0] / number) - img.shape[0]\n",
    "\n",
    "    if w > 0 or h > 0:\n",
    "        padded = cv2.copyMakeBorder(\n",
    "            img, 0, h, 0, w, borderType=cv2.BORDER_REPLICATE)\n",
    "        return padded\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "\n",
    "class TransformerReshapeTransform:\n",
    "    def __init__(self):\n",
    "        self.input_tensor_shape = None\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        print(self.input_tensor_shape)\n",
    "        result = torch.nn.ReLU()(tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "                                self.input_tensor_shape[2] // 16,\n",
    "                                self.input_tensor_shape[3] // 16,\n",
    "                                tensor.size(2)))\n",
    "        # Bring the channels to the first dimension,\n",
    "        # like in CNNs.\n",
    "        result = result.transpose(2, 3).transpose(1, 2)\n",
    "        return result\n",
    "\n",
    "print(cluster_model.cluster_centers_.shape)\n",
    "\n",
    "img_path = r\"/home/gildenbj/a.png\"\n",
    "img = np.array(Image.open(img_path))\n",
    "orig_shape = img.shape\n",
    "img = pad_divisible(img)\n",
    "rgb_img_float = np.float32(img) / 255\n",
    "input_tensor = preprocess_image(rgb_img_float,\n",
    "                                mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "\n",
    "img_path2 = r\"/home/gildenbj/b.png\"\n",
    "img2 = np.array(Image.open(img_path2))\n",
    "orig_shape2 = img2.shape\n",
    "img2 = pad_divisible(img2)\n",
    "\n",
    "rgb_img_float2 = np.float32(img2) / 255\n",
    "input_tensor2 = preprocess_image(rgb_img_float2,\n",
    "                                mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"uni\"\n",
    "\n",
    "if model_name == \"gigapath\":\n",
    "    model = timm.create_model(\"hf_hub:prov-gigapath/prov-gigapath\", pretrained=True, dynamic_img_size=True)\n",
    "    target_layer = model.blocks[-1]\n",
    "    transform = TransformerReshapeTransform()\n",
    "\n",
    "if model_name == \"resnet50\":\n",
    "    concepts = np.load(\"/home/gildenbj/dev/dff_seg/concepts_resnet50_concepts.npy\")\n",
    "    model = resnet50(pretrained=True)\n",
    "    target_layer = model.layer4\n",
    "    transform = None\n",
    "elif model_name == \"uni\":\n",
    "    model = timm.create_model(\"hf-hub:MahmoodLab/uni\", pretrained=True, init_values=1e-5, dynamic_img_size=True)\n",
    "    target_layer = model.blocks[-1]    \n",
    "    transform = TransformerReshapeTransform()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "unsupervised_seg = DFFSeg(\n",
    "    model=model,\n",
    "    target_layer=target_layer,\n",
    "    reshape_transform=transform,\n",
    "    scale_before_argmax=True\n",
    ")\n",
    "\n",
    "unsupervised_seg.reshape_transform.input_tensor_shape = input_tensor.shape\n",
    "concepts = cluster_model.cluster_centers_\n",
    "concepts[concepts < 0] = 0\n",
    "print(concepts)\n",
    "print(concepts.min(), concepts.max())\n",
    "segmentation = unsupervised_seg.predict_project_concepts(input_tensor, concepts)\n",
    "visualization = show_segmentation_on_image(\n",
    "    img=img,\n",
    "    segmentation=segmentation,\n",
    "    image_weight=0.7,\n",
    "    n_categories=len(concepts))\n",
    "\n",
    "unsupervised_seg.reshape_transform.input_tensor_shape = input_tensor2.shape\n",
    "segmentation2 = unsupervised_seg.predict_project_concepts(input_tensor2, concepts)\n",
    "visualization2 = show_segmentation_on_image(\n",
    "    img=img2,\n",
    "    segmentation=segmentation2,\n",
    "    image_weight=0.7,\n",
    "    n_categories=len(concepts))\n",
    "\n",
    "visualization = visualization[:orig_shape[0], :orig_shape[1], :]\n",
    "visualization2 = visualization2[:orig_shape2[0], :orig_shape2[1], :]\n",
    "\n",
    "display(Image.fromarray(visualization))\n",
    "display(Image.fromarray(visualization2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
