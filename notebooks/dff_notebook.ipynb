{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients\n",
    "import torch\n",
    "import denseCRF\n",
    "from sklearn.decomposition import NMF, non_negative_factorization\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def show_segmentation_on_image(\n",
    "        img: np.uint8,\n",
    "        segmentation: np.ndarray,\n",
    "        colors: list[np.ndarray] = None,\n",
    "        n_categories: int = None,\n",
    "        image_weight: float = 0.5\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Color code the different component heatmaps on top of the image.\n",
    "\n",
    "    Since different factorization component heatmaps can overlap in principle,\n",
    "    we need a strategy to decide how to deal with the overlaps.\n",
    "    This keeps the component that has a higher value in its heatmap.\n",
    "\n",
    "    :param img: The base image in RGB format.\n",
    "    :param segmentation: A numpy array with category indices per pixel.\n",
    "    :param colors: List of R, G, B colors to be used for the components. \n",
    "                   If None, will use the gist_rainbow colormap as a default.\n",
    "    :param n_categories: Number of categories in the segmentation.\n",
    "    :param image_weight: The final result is image_weight * img + (1-image_weight) * visualization.\n",
    "    :return: The visualized image.\n",
    "\n",
    "    \"\"\"\n",
    "    float_img = np.float32(img) / 255\n",
    "    categories = list(range(n_categories))\n",
    "    if colors is None:\n",
    "        # taken from https://github.com/edocollins/DFF/blob/master/utils.py\n",
    "        _cmap = plt.cm.get_cmap('gist_rainbow')\n",
    "        colors = [\n",
    "            np.array(\n",
    "                _cmap(i)) for i in np.arange(\n",
    "                0,\n",
    "                1,\n",
    "                1.0 /\n",
    "                len(categories))]\n",
    "\n",
    "    mask = np.zeros(shape=(img.shape[0], img.shape[1], 3))\n",
    "    for category in categories:\n",
    "        mask[segmentation == category] = colors[category][:3]\n",
    "\n",
    "    result = float_img * image_weight + mask * (1 - image_weight)\n",
    "    result = np.uint8(result * 255)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def dff(activations: np.ndarray, n_components: int = 5):\n",
    "    \"\"\"Compute Deep Feature Factorization on a 2d Activations tensor.\n",
    "\n",
    "    :param activations: A numpy array of shape batch x channels x height x width\n",
    "    :param n_components: The number of components for the non negative matrix factorization\n",
    "    :return: A tuple of the concepts (a numpy array with shape channels x components),\n",
    "              and the explanation heatmaps (a numpy arary with shape batch x height x width)\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, __, h, w = activations.shape\n",
    "    reshaped_activations = activations.transpose((1, 0, 2, 3))\n",
    "    reshaped_activations[np.isnan(reshaped_activations)] = 0\n",
    "    reshaped_activations = reshaped_activations.reshape(\n",
    "        reshaped_activations.shape[0], -1)\n",
    "    reshaped_activations = reshaped_activations\n",
    "    model = NMF(n_components=n_components, init='random', random_state=0)\n",
    "    W = model.fit_transform(reshaped_activations)\n",
    "    H = model.components_\n",
    "    concepts = W\n",
    "    explanations = H.reshape(n_components, batch_size, h, w)\n",
    "    explanations = explanations.transpose((1, 0, 2, 3))\n",
    "\n",
    "    return concepts, explanations\n",
    "\n",
    "\n",
    "def densecrf(\n",
    "        I: np.ndarray,\n",
    "        P: np.ndarray,\n",
    "        params: tuple[float, float, float, float, float, int],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Applying densecrf.\n",
    "\n",
    "    :param I: A numpy array of shape [H, W, C], where C should be 3.\n",
    "               type of I should be np.uint8, and the values are in [0, 255]\n",
    "    :param P: A probability map of shape [H, W, L], where L is the number of classes\n",
    "               type of P should be np.float32\n",
    "    :param params: A tuple giving parameters of CRF (w1, alpha, beta, w2, gamma, it).\n",
    "    :return: A numpy array of shape [H, W], where pixel values represent class indices.\n",
    "\n",
    "    \"\"\"\n",
    "    out = denseCRF.densecrf(I, P, params) \n",
    "    return out\n",
    "\n",
    "\n",
    "def densecrf_on_image(\n",
    "        image: np.ndarray,\n",
    "        prob: np.ndarray,\n",
    "        w1: float = 10.0,\n",
    "        w2: float = 3.0,\n",
    "        alpha: float = 80,\n",
    "        beta: float = 13,\n",
    "        gamma: float = 3,\n",
    "        it: int = 5,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Applying densecrf on image, given the segmentation probabilities.\n",
    "\n",
    "    :param iamge: Input rgb image.\n",
    "    :param prob: Probability mask.\n",
    "    :param w1: Weight of bilateral term, e.g. 10.0\n",
    "    :param alpha: Spatial distance std, e.g., 80\n",
    "    :param beta: Rgb value std, e.g., 15\n",
    "    :param w2: Weight of spatial term, e.g., 3.0\n",
    "    :param gamma: Spatial distance std for spatial term, e.g., 3\n",
    "    :param it: Iteration number, e.g., 5\n",
    "    :return: A numpy array of shape [H, W], where pixel values represent class indices.\n",
    "\n",
    "    \"\"\"\n",
    "    I  = image\n",
    "    Iq = np.asarray(I)\n",
    "    prob = prob / prob.sum(axis=-1)[:, :, None]\n",
    "\n",
    "    param = (w1, alpha, beta, w2, gamma, it)\n",
    "    lab = densecrf(Iq, prob, param)\n",
    "    lab = np.array(Image.fromarray(lab))\n",
    "    return lab\n",
    "\n",
    "\n",
    "\n",
    "class DFFSeg:\n",
    "    \"\"\"\n",
    "    A class to perform Deep Feature Factorization (DFF) based segmentation on images.\n",
    "\n",
    "    :param model: The model to be used for generating activations.\n",
    "    :param target_layer: The layer of the model from which activations are extracted.\n",
    "    :param n_concepts: The number of concepts for the non-negative matrix factorization.\n",
    "    :param reshape_transform: A function to reshape the activations, defaults to None.\n",
    "    :param random_state: Random state for reproducibility, defaults to 0.\n",
    "    :param crf_smoothing: Whether to apply CRF smoothing, defaults to False.\n",
    "    :param w1: Weight of the bilateral term for CRF, defaults to 10.0.\n",
    "    :param w2: Weight of the spatial term for CRF, defaults to 3.0.\n",
    "    :param alpha: Spatial distance std for CRF, defaults to 80.\n",
    "    :param beta: RGB value std for CRF, defaults to 13.\n",
    "    :param gamma: Spatial distance std for the spatial term in CRF, defaults to 3.\n",
    "    :param it: Iteration number for CRF, defaults to 5.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            target_layer,\n",
    "            n_concepts: int,\n",
    "            reshape_transform: Callable = None,\n",
    "            random_state: int = 0,\n",
    "            crf_smoothing: bool = False,\n",
    "            w1: float = 10.0,\n",
    "            w2: float = 3.0,\n",
    "            alpha: float = 80,\n",
    "            beta: float = 13,\n",
    "            gamma: float = 3,\n",
    "            it: int = 5,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.n_concepts = n_concepts\n",
    "        self.reshape_transform = reshape_transform\n",
    "        self.random_state = random_state\n",
    "        self.crf_smoothing = crf_smoothing\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.it = it\n",
    "\n",
    "    def fit_predict(self, input_tensor: torch.tensor) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fit the model on the input tensor and predict the segmentation.\n",
    "\n",
    "        :param input_tensor: The input tensor.\n",
    "        :return: The predicted segmentation as a numpy array.\n",
    "\n",
    "        \"\"\"\n",
    "        self.fit(input_tensor=input_tensor)\n",
    "        return self.predict(input_tensor)\n",
    "\n",
    "    def predict(self, input_tensor: torch.tensor):\n",
    "        \"\"\"\n",
    "        Predict the segmentation for the given input tensor.\n",
    "\n",
    "        :param input_tensor: The input tensor.\n",
    "        :return: The predicted segmentation as a numpy array.\n",
    "\n",
    "        \"\"\"\n",
    "        activations_and_grads = ActivationsAndGradients(\n",
    "                    self.model, [self.target_layer], self.reshape_transform)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            activations_and_grads(input_tensor)\n",
    "            activations = activations_and_grads.activations[0].cpu(\n",
    "            ).numpy()\n",
    "\n",
    "        activations = activations[0].transpose((1, 2, 0))\n",
    "        vector = activations.reshape(-1, activations.shape[-1])\n",
    "        \n",
    "        w, __, __ = non_negative_factorization(\n",
    "            X=vector,\n",
    "            H=self.concepts.transpose(),\n",
    "            W=None,\n",
    "            n_components=self.n_concepts,\n",
    "            update_H=False,\n",
    "            random_state=self.random_state,\n",
    "            max_iter=10000,\n",
    "        )\n",
    "\n",
    "        w = w.reshape((activations.shape[0], activations.shape[1], -1))\n",
    "        w_for_resize = torch.tensor(w.transpose((2,0,1))[None, :, :, :])  # Add batch dimension\n",
    "        size = (input_tensor.shape[2], input_tensor.shape[3])\n",
    "        w_resized = torch.nn.functional.interpolate(w_for_resize, size, mode='bilinear')[0].numpy().transpose((1, 2, 0))\n",
    "        \n",
    "        if self.crf_smoothing:\n",
    "            segmentation = densecrf_on_image(np.uint8(input_tensor[0].cpu().numpy()).transpose(1,2,0), w_resized)\n",
    "        else:\n",
    "            w_resized = w_resized.argmax(axis=-1)\n",
    "            segmentation = np.array(Image.fromarray(np.uint8(w_resized)).resize((input_tensor.shape[3], input_tensor.shape[2])))\n",
    "        return segmentation\n",
    "        \n",
    "    def fit(self, input_tensor: torch.tensor) -> None:\n",
    "        \"\"\"\n",
    "        Fit the model on the input tensor to compute the concepts.\n",
    "\n",
    "        :param input_tensor: The input tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        activations_and_grads = ActivationsAndGradients(\n",
    "                    self.model, [self.target_layer], self.reshape_transform)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            activations_and_grads(input_tensor)\n",
    "            activations = activations_and_grads.activations[0].cpu().numpy()\n",
    "            concepts, __ = dff(activations, self.n_concepts)\n",
    "\n",
    "        self.concepts = concepts\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to C:\\Users\\User\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "<class 'timm.models.vision_transformer.VisionTransformer'>\n",
      "<class 'timm.models.vision_transformer.Block'>\n",
      "result: torch.Size([1, 1024, 63, 89])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\decomposition\\_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: torch.Size([1, 1024, 63, 89])\n",
      "result: torch.Size([1, 1024, 63, 89])\n",
      "result: torch.Size([1, 1024, 63, 89])\n",
      "result: torch.Size([1, 1024, 63, 89])\n",
      "result: torch.Size([1, 1024, 63, 89])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam.utils.image import preprocess_image\n",
    "from torchvision.models import resnet50\n",
    "from functools import partial\n",
    "import timm\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "NUM_CONCEPTS = 20\n",
    "\n",
    "\n",
    "def uni_model_transform(tensor, width, height):\n",
    "    result = torch.nn.ReLU()(tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "                            height,\n",
    "                            width,\n",
    "                            tensor.size(2)))\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    print(f\"result: {result.shape}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "img_path = r\"D:\\BCSSDataset\\images\\TCGA-A1-A0SP-DX1_xmin6798_ymin53719_MAG-10.00.png\"\n",
    "img = np.array(Image.open(img_path))\n",
    "img = img[:16*(img.shape[0] // 16), :16*(img.shape[1] // 16), :]\n",
    "# img = img[:256, :256, :]\n",
    "rgb_img_float = np.float32(img) / 255\n",
    "input_tensor = preprocess_image(rgb_img_float,\n",
    "                                mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "img_path2 = r\"D:\\BCSSDataset\\images\\TCGA-A2-A0CM-DX1_xmin18562_ymin56852_MAG-10.00.png\"\n",
    "img2 = np.array(Image.open(img_path2))\n",
    "img2 = img[:16*(img2.shape[0] // 16), :16*(img2.shape[1] // 16), :]\n",
    "rgb_img_float2 = np.float32(img2) / 255\n",
    "input_tensor2 = preprocess_image(rgb_img_float2,\n",
    "                                mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "\n",
    "model_name = \"uni\"\n",
    "\n",
    "if model_name == \"resnet\":\n",
    "    model = resnet50(pretrained=True)\n",
    "    target_layer = model.layer3\n",
    "    reshape_transform = None\n",
    "elif model_name == \"uni\":\n",
    "    token = \"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\"\n",
    "    login(token=token)\n",
    "    model = timm.create_model(\"hf-hub:MahmoodLab/uni\", pretrained=True, init_values=1e-5, dynamic_img_size=True)\n",
    "    target_layer = model.blocks[-1]\n",
    "    reshape_transform = partial(\n",
    "        uni_model_transform,\n",
    "        width=input_tensor.shape[3]//16,\n",
    "        height=input_tensor.shape[2]//16,\n",
    "    )\n",
    "\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "unsupervised_seg = DFFSeg(\n",
    "    model=model,\n",
    "    target_layer=target_layer,\n",
    "    n_concepts=NUM_CONCEPTS,\n",
    "    reshape_transform=reshape_transform\n",
    ")\n",
    "\n",
    "input_tensor = input_tensor.cuda()\n",
    "input_tensor2 = input_tensor2.cuda()\n",
    "unsupervised_seg.fit(input_tensor)\n",
    "segmentation = unsupervised_seg.predict(input_tensor=input_tensor)\n",
    "unsupervised_seg.crf_smoothing = True\n",
    "segmentation2 = unsupervised_seg.predict(input_tensor=input_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img2: (960, 1264, 3), segmentation2: (1008, 1424)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 960 but corresponding boolean dimension is 1008",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m visualization \u001b[38;5;241m=\u001b[39m show_segmentation_on_image(\n\u001b[0;32m      2\u001b[0m     img\u001b[38;5;241m=\u001b[39mimg,\n\u001b[0;32m      3\u001b[0m     segmentation\u001b[38;5;241m=\u001b[39msegmentation,\n\u001b[0;32m      4\u001b[0m     image_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[0;32m      5\u001b[0m     n_categories\u001b[38;5;241m=\u001b[39mNUM_CONCEPTS,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg2\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, segmentation2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegmentation2\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m visualization2 \u001b[38;5;241m=\u001b[39m \u001b[43mshow_segmentation_on_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegmentation2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_categories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_CONCEPTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# display(Image.fromarray(visualization))\u001b[39;00m\n\u001b[0;32m     15\u001b[0m display(Image\u001b[38;5;241m.\u001b[39mfromarray(np\u001b[38;5;241m.\u001b[39mhstack((visualization, visualization2))))\n",
      "Cell \u001b[1;32mIn[27], line 48\u001b[0m, in \u001b[0;36mshow_segmentation_on_image\u001b[1;34m(img, segmentation, colors, n_categories, image_weight)\u001b[0m\n\u001b[0;32m     46\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m categories:\n\u001b[1;32m---> 48\u001b[0m     \u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43msegmentation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m colors[category][:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m     50\u001b[0m result \u001b[38;5;241m=\u001b[39m float_img \u001b[38;5;241m*\u001b[39m image_weight \u001b[38;5;241m+\u001b[39m mask \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m image_weight)\n\u001b[0;32m     51\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8(result \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 960 but corresponding boolean dimension is 1008"
     ]
    }
   ],
   "source": [
    "visualization = show_segmentation_on_image(\n",
    "    img=img,\n",
    "    segmentation=segmentation,\n",
    "    image_weight=0.7,\n",
    "    n_categories=NUM_CONCEPTS,\n",
    ")\n",
    "print(f\"img2: {img2.shape}, segmentation2: {segmentation2.shape}\")\n",
    "visualization2 = show_segmentation_on_image(\n",
    "    img=img2,\n",
    "    segmentation=segmentation2,\n",
    "    image_weight=0.7,\n",
    "    n_categories=NUM_CONCEPTS,\n",
    ")\n",
    "# display(Image.fromarray(visualization))\n",
    "display(Image.fromarray(np.hstack((visualization, visualization2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block(\n",
       "  (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (attn): Attention(\n",
       "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "    (q_norm): Identity()\n",
       "    (k_norm): Identity()\n",
       "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (ls1): LayerScale()\n",
       "  (drop_path1): Identity()\n",
       "  (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (mlp): Mlp(\n",
       "    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (act): GELU(approximate='none')\n",
       "    (drop1): Dropout(p=0.0, inplace=False)\n",
       "    (norm): Identity()\n",
       "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (drop2): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (ls2): LayerScale()\n",
       "  (drop_path2): Identity()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[-1].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
