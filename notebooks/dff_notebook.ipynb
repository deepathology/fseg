{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/gildenbj/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "token = \"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\"\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m concepts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m/home/gildenbj/dev/dff_seg/concepts_uni_concepts.npy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m num_concepts \u001b[39m=\u001b[39m concepts\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 92\u001b[0m model \u001b[39m=\u001b[39m timm\u001b[39m.\u001b[39;49mcreate_model(\u001b[39m\"\u001b[39;49m\u001b[39mhf-hub:MahmoodLab/uni\u001b[39;49m\u001b[39m\"\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, init_values\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m, dynamic_img_size\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     93\u001b[0m target_layer \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mblocks[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     95\u001b[0m transform \u001b[39m=\u001b[39m TransformerReshapeTransform()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/timm/models/_factory.py:117\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m create_fn \u001b[39m=\u001b[39m model_entrypoint(model_name)\n\u001b[1;32m    116\u001b[0m \u001b[39mwith\u001b[39;00m set_layer_config(scriptable\u001b[39m=\u001b[39mscriptable, exportable\u001b[39m=\u001b[39mexportable, no_jit\u001b[39m=\u001b[39mno_jit):\n\u001b[0;32m--> 117\u001b[0m     model \u001b[39m=\u001b[39m create_fn(\n\u001b[1;32m    118\u001b[0m         pretrained\u001b[39m=\u001b[39;49mpretrained,\n\u001b[1;32m    119\u001b[0m         pretrained_cfg\u001b[39m=\u001b[39;49mpretrained_cfg,\n\u001b[1;32m    120\u001b[0m         pretrained_cfg_overlay\u001b[39m=\u001b[39;49mpretrained_cfg_overlay,\n\u001b[1;32m    121\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    122\u001b[0m     )\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m checkpoint_path:\n\u001b[1;32m    125\u001b[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/timm/models/vision_transformer.py:2182\u001b[0m, in \u001b[0;36mvit_large_patch16_224\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   2178\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\u001b[39;00m\n\u001b[1;32m   2179\u001b[0m \u001b[39mImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\u001b[39;00m\n\u001b[1;32m   2180\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2181\u001b[0m model_args \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(patch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, embed_dim\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, depth\u001b[39m=\u001b[39m\u001b[39m24\u001b[39m, num_heads\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n\u001b[0;32m-> 2182\u001b[0m model \u001b[39m=\u001b[39m _create_vision_transformer(\u001b[39m'\u001b[39;49m\u001b[39mvit_large_patch16_224\u001b[39;49m\u001b[39m'\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49mpretrained, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(model_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m   2183\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/timm/models/vision_transformer.py:2033\u001b[0m, in \u001b[0;36m_create_vision_transformer\u001b[0;34m(variant, pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39msiglip\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m variant \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mglobal_pool\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmap\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   2031\u001b[0m     strict \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 2033\u001b[0m \u001b[39mreturn\u001b[39;00m build_model_with_cfg(\n\u001b[1;32m   2034\u001b[0m     VisionTransformer,\n\u001b[1;32m   2035\u001b[0m     variant,\n\u001b[1;32m   2036\u001b[0m     pretrained,\n\u001b[1;32m   2037\u001b[0m     pretrained_filter_fn\u001b[39m=\u001b[39;49m_filter_fn,\n\u001b[1;32m   2038\u001b[0m     pretrained_strict\u001b[39m=\u001b[39;49mstrict,\n\u001b[1;32m   2039\u001b[0m     feature_cfg\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(out_indices\u001b[39m=\u001b[39;49mout_indices, feature_cls\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgetter\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m   2040\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2041\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/timm/models/_builder.py:418\u001b[0m, in \u001b[0;36mbuild_model_with_cfg\u001b[0;34m(model_cls, variant, pretrained, pretrained_cfg, pretrained_cfg_overlay, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, kwargs_filter, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m num_classes_pretrained \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m features \u001b[39melse\u001b[39;00m \u001b[39mgetattr\u001b[39m(model, \u001b[39m'\u001b[39m\u001b[39mnum_classes\u001b[39m\u001b[39m'\u001b[39m, kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mnum_classes\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1000\u001b[39m))\n\u001b[1;32m    417\u001b[0m \u001b[39mif\u001b[39;00m pretrained:\n\u001b[0;32m--> 418\u001b[0m     load_pretrained(\n\u001b[1;32m    419\u001b[0m         model,\n\u001b[1;32m    420\u001b[0m         pretrained_cfg\u001b[39m=\u001b[39;49mpretrained_cfg,\n\u001b[1;32m    421\u001b[0m         num_classes\u001b[39m=\u001b[39;49mnum_classes_pretrained,\n\u001b[1;32m    422\u001b[0m         in_chans\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39min_chans\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m3\u001b[39;49m),\n\u001b[1;32m    423\u001b[0m         filter_fn\u001b[39m=\u001b[39;49mpretrained_filter_fn,\n\u001b[1;32m    424\u001b[0m         strict\u001b[39m=\u001b[39;49mpretrained_strict,\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    427\u001b[0m \u001b[39m# Wrap the model in a feature extraction module if enabled\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39mif\u001b[39;00m features:\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/timm/models/_builder.py:196\u001b[0m, in \u001b[0;36mload_pretrained\u001b[0;34m(model, pretrained_cfg, num_classes, in_chans, filter_fn, strict)\u001b[0m\n\u001b[1;32m    194\u001b[0m             state_dict \u001b[39m=\u001b[39m load_state_dict_from_hf(\u001b[39m*\u001b[39mpretrained_loc)\n\u001b[1;32m    195\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict_from_hf(pretrained_loc)\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     model_name \u001b[39m=\u001b[39m pretrained_cfg\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39marchitecture\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mthis model\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/timm/models/_hub.py:190\u001b[0m, in \u001b[0;36mload_state_dict_from_hf\u001b[0;34m(model_id, filename)\u001b[0m\n\u001b[1;32m    188\u001b[0m cached_file \u001b[39m=\u001b[39m hf_hub_download(hf_model_id, filename\u001b[39m=\u001b[39mfilename, revision\u001b[39m=\u001b[39mhf_revision)\n\u001b[1;32m    189\u001b[0m _logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mmodel_id\u001b[39m}\u001b[39;00m\u001b[39m] Safe alternative not found for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Loading weights using default pytorch.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 190\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(cached_file, map_location\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1051\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1021\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.9/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m    995\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49m_UntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m     \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[1;32m   1001\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1002\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import dff_seg.dff_seg\n",
    "importlib.reload(dff_seg.dff_seg)\n",
    "from dff_seg.dff_seg import DFFSeg, show_segmentation_on_image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam.utils.image import preprocess_image\n",
    "from torchvision.models import resnet50\n",
    "from functools import partial\n",
    "import torch\n",
    "import cv2\n",
    "import timm\n",
    "import math\n",
    "\n",
    "\n",
    "def pad_divisible(img: np.ndarray, number: int = 16) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pad the image shape to be a multiple of number\n",
    "\n",
    "    :param img: Input image.\n",
    "    :param scale: Scale factor.\n",
    "    :return: Padded input image.\n",
    "    \"\"\"\n",
    "    w = number * math.ceil(img.shape[1] / number) - img.shape[1]\n",
    "    h = number * math.ceil(img.shape[0] / number) - img.shape[0]\n",
    "\n",
    "    if w > 0 or h > 0:\n",
    "        padded = cv2.copyMakeBorder(\n",
    "            img, 0, h, 0, w, borderType=cv2.BORDER_REPLICATE)\n",
    "        return padded\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "\n",
    "class TransformerReshapeTransform:\n",
    "    def __init__(self):\n",
    "        self.input_tensor_shape = None\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        print(self.input_tensor_shape)\n",
    "        result = torch.nn.ReLU()(tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "                                self.input_tensor_shape[2] // 16,\n",
    "                                self.input_tensor_shape[3] // 16,\n",
    "                                tensor.size(2)))\n",
    "        # Bring the channels to the first dimension,\n",
    "        # like in CNNs.\n",
    "        result = result.transpose(2, 3).transpose(1, 2)\n",
    "        return result\n",
    "\n",
    "num_concepts = 20\n",
    "img_path = r\"/home/gildenbj/a.png\"\n",
    "img = np.array(Image.open(img_path))\n",
    "orig_shape = img.shape\n",
    "img = pad_divisible(img)\n",
    "rgb_img_float = np.float32(img) / 255\n",
    "input_tensor = preprocess_image(rgb_img_float,\n",
    "                                mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "\n",
    "img_path2 = r\"/home/gildenbj/b.png\"\n",
    "img2 = np.array(Image.open(img_path2))\n",
    "orig_shape2 = img2.shape\n",
    "img2 = pad_divisible(img2)\n",
    "\n",
    "rgb_img_float2 = np.float32(img2) / 255\n",
    "input_tensor2 = preprocess_image(rgb_img_float2,\n",
    "                                mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"uni\"\n",
    "\n",
    "if model_name == \"gigapath\":\n",
    "    model = timm.create_model(\"hf_hub:prov-gigapath/prov-gigapath\", pretrained=True, dynamic_img_size=True)\n",
    "    target_layer = model.blocks[-1]\n",
    "    concepts = np.load(\"/home/gildenbj/dev/dff_seg/concepts_gigapath_concepts.npy\")\n",
    "    num_concepts = concepts.shape[0]\n",
    "    transform = TransformerReshapeTransform()\n",
    "\n",
    "if model_name == \"resnet50\":\n",
    "    concepts = np.load(\"/home/gildenbj/dev/dff_seg/concepts_resnet50_concepts.npy\")\n",
    "    num_concepts = concepts.shape[0]\n",
    "    model = resnet50(pretrained=True)\n",
    "    target_layer = model.layer4\n",
    "    transform = None\n",
    "elif model_name == \"uni\":\n",
    "    concepts = np.load(\"/home/gildenbj/dev/dff_seg/concepts_uni_concepts.npy\")\n",
    "    num_concepts = concepts.shape[0]\n",
    "    model = timm.create_model(\"hf-hub:MahmoodLab/uni\", pretrained=True, init_values=1e-5, dynamic_img_size=True)\n",
    "    target_layer = model.blocks[-1]\n",
    "    \n",
    "    transform = TransformerReshapeTransform()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "unsupervised_seg = DFFSeg(\n",
    "    model=model,\n",
    "    target_layer=target_layer,\n",
    "    n_concepts=num_concepts,\n",
    "    reshape_transform=transform,\n",
    "    concepts=concepts,\n",
    "    scale_before_argmax=True\n",
    ")\n",
    "\n",
    "unsupervised_seg.reshape_transform.input_tensor_shape = input_tensor.shape\n",
    "\n",
    "#unsupervised_seg.partial_fit(input_tensor=input_tensor)\n",
    "segmentation = unsupervised_seg.predict(input_tensor=input_tensor)\n",
    "visualization = show_segmentation_on_image(\n",
    "    img=img,\n",
    "    segmentation=segmentation,\n",
    "    image_weight=0.7,\n",
    "    n_categories=num_concepts)\n",
    "\n",
    "unsupervised_seg.reshape_transform.input_tensor_shape = input_tensor2.shape\n",
    "\n",
    "segmentation2 = unsupervised_seg.predict(input_tensor=input_tensor2)\n",
    "visualization2 = show_segmentation_on_image(\n",
    "    img=img2,\n",
    "    segmentation=segmentation2,\n",
    "    image_weight=0.7,\n",
    "    n_categories=num_concepts)\n",
    "\n",
    "visualization = visualization[:orig_shape[0], :orig_shape[1], :]\n",
    "visualization2 = visualization2[:orig_shape2[0], :orig_shape2[1], :]\n",
    "\n",
    "display(Image.fromarray(visualization))\n",
    "display(Image.fromarray(visualization2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
