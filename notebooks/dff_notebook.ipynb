{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/gildenbj/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "token = \"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\"\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1024)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "cluster_model = joblib.load(\"/gstore/data/dp_labs/tcga/1000_per_slide/embeddings/_uni/5239_m_10_0_tile_256_step_256_mask_atd_image_format_png_NONE_embed_imagenet_backbone_backbone_uni_model_name_uni_aug_0_stain_None/aggregated/clusters_model_128.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1024)\n",
      "torch.Size([1, 3, 1024, 1440])\n",
      "(1024, 20) (1, 20, 64, 90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gildenbj/miniconda3/envs/py38/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DFFSeg' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 119\u001b[0m\n\u001b[1;32m    112\u001b[0m visualization \u001b[39m=\u001b[39m show_segmentation_on_image(\n\u001b[1;32m    113\u001b[0m     img\u001b[39m=\u001b[39mimg,\n\u001b[1;32m    114\u001b[0m     segmentation\u001b[39m=\u001b[39msegmentation,\n\u001b[1;32m    115\u001b[0m     image_weight\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m,\n\u001b[1;32m    116\u001b[0m     n_categories\u001b[39m=\u001b[39mnum_concepts)\n\u001b[1;32m    118\u001b[0m unsupervised_seg\u001b[39m.\u001b[39mreshape_transform\u001b[39m.\u001b[39minput_tensor_shape \u001b[39m=\u001b[39m input_tensor2\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 119\u001b[0m segmentation2 \u001b[39m=\u001b[39m unsupervised_seg\u001b[39m.\u001b[39;49mpredict(input_tensor\u001b[39m=\u001b[39minput_tensor2, concepts\u001b[39m=\u001b[39mconcepts)\n\u001b[1;32m    120\u001b[0m visualization2 \u001b[39m=\u001b[39m show_segmentation_on_image(\n\u001b[1;32m    121\u001b[0m     img\u001b[39m=\u001b[39mimg2,\n\u001b[1;32m    122\u001b[0m     segmentation\u001b[39m=\u001b[39msegmentation2,\n\u001b[1;32m    123\u001b[0m     image_weight\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m,\n\u001b[1;32m    124\u001b[0m     n_categories\u001b[39m=\u001b[39mnum_concepts)\n\u001b[1;32m    126\u001b[0m visualization \u001b[39m=\u001b[39m visualization[:orig_shape[\u001b[39m0\u001b[39m], :orig_shape[\u001b[39m1\u001b[39m], :]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DFFSeg' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import dff_seg.dff_seg\n",
    "importlib.reload(dff_seg.dff_seg)\n",
    "from dff_seg.dff_seg import DFFSeg, show_segmentation_on_image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam.utils.image import preprocess_image\n",
    "from torchvision.models import resnet50\n",
    "from functools import partial\n",
    "import torch\n",
    "import cv2\n",
    "import timm\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "cluster_model = joblib.load(\"/gstore/data/dp_labs/tcga/1000_per_slide/embeddings/_uni/5239_m_10_0_tile_256_step_256_mask_atd_image_format_png_NONE_embed_imagenet_backbone_backbone_uni_model_name_uni_aug_0_stain_None/aggregated/clusters_model_32.joblib\")\n",
    "\n",
    "def pad_divisible(img: np.ndarray, number: int = 16) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pad the image shape to be a multiple of number\n",
    "\n",
    "    :param img: Input image.\n",
    "    :param scale: Scale factor.\n",
    "    :return: Padded input image.\n",
    "    \"\"\"\n",
    "    w = number * math.ceil(img.shape[1] / number) - img.shape[1]\n",
    "    h = number * math.ceil(img.shape[0] / number) - img.shape[0]\n",
    "\n",
    "    if w > 0 or h > 0:\n",
    "        padded = cv2.copyMakeBorder(\n",
    "            img, 0, h, 0, w, borderType=cv2.BORDER_REPLICATE)\n",
    "        return padded\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "\n",
    "class TransformerReshapeTransform:\n",
    "    def __init__(self):\n",
    "        self.input_tensor_shape = None\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        print(self.input_tensor_shape)\n",
    "        result = torch.nn.ReLU()(tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "                                self.input_tensor_shape[2] // 16,\n",
    "                                self.input_tensor_shape[3] // 16,\n",
    "                                tensor.size(2)))\n",
    "        # Bring the channels to the first dimension,\n",
    "        # like in CNNs.\n",
    "        result = result.transpose(2, 3).transpose(1, 2)\n",
    "        return result\n",
    "\n",
    "print(cluster_model.cluster_centers_.shape)\n",
    "\n",
    "img_path = r\"/home/gildenbj/a.png\"\n",
    "img = np.array(Image.open(img_path))\n",
    "orig_shape = img.shape\n",
    "img = pad_divisible(img)\n",
    "rgb_img_float = np.float32(img) / 255\n",
    "input_tensor = preprocess_image(rgb_img_float,\n",
    "                                mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "\n",
    "img_path2 = r\"/home/gildenbj/b.png\"\n",
    "img2 = np.array(Image.open(img_path2))\n",
    "orig_shape2 = img2.shape\n",
    "img2 = pad_divisible(img2)\n",
    "\n",
    "rgb_img_float2 = np.float32(img2) / 255\n",
    "input_tensor2 = preprocess_image(rgb_img_float2,\n",
    "                                mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"uni\"\n",
    "concepts = cluster_model.cluster_centers_.transpose()\n",
    "\n",
    "if model_name == \"gigapath\":\n",
    "    model = timm.create_model(\"hf_hub:prov-gigapath/prov-gigapath\", pretrained=True, dynamic_img_size=True)\n",
    "    target_layer = model.blocks[-1]\n",
    "    concepts = np.load(\"/home/gildenbj/dev/dff_seg/concepts_gigapath_concepts.npy\")\n",
    "    num_concepts = concepts.shape[0]\n",
    "    transform = TransformerReshapeTransform()\n",
    "\n",
    "if model_name == \"resnet50\":\n",
    "    concepts = np.load(\"/home/gildenbj/dev/dff_seg/concepts_resnet50_concepts.npy\")\n",
    "    num_concepts = concepts.shape[0]\n",
    "    model = resnet50(pretrained=True)\n",
    "    target_layer = model.layer4\n",
    "    transform = None\n",
    "elif model_name == \"uni\":\n",
    "    concepts = np.load(\"/home/gildenbj/dev/dff_seg/concepts_uni_concepts.npy\")\n",
    "    num_concepts = concepts.shape[0]\n",
    "    model = timm.create_model(\"hf-hub:MahmoodLab/uni\", pretrained=True, init_values=1e-5, dynamic_img_size=True)\n",
    "    target_layer = model.blocks[-1]\n",
    "    \n",
    "    transform = TransformerReshapeTransform()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "unsupervised_seg = DFFSeg(\n",
    "    model=model,\n",
    "    target_layer=target_layer,\n",
    "    reshape_transform=transform,\n",
    "    scale_before_argmax=True\n",
    ")\n",
    "\n",
    "unsupervised_seg.reshape_transform.input_tensor_shape = input_tensor.shape\n",
    "segmentation = unsupervised_seg.predict_clustering(input_tensor=input_tensor, clusters=concepts)\n",
    "visualization = show_segmentation_on_image(\n",
    "    img=img,\n",
    "    segmentation=segmentation,\n",
    "    image_weight=0.7,\n",
    "    n_categories=num_concepts)\n",
    "\n",
    "unsupervised_seg.reshape_transform.input_tensor_shape = input_tensor2.shape\n",
    "segmentation2 = unsupervised_seg.predict_clustering(input_tensor=input_tensor2, clusters=concepts)\n",
    "visualization2 = show_segmentation_on_image(\n",
    "    img=img2,\n",
    "    segmentation=segmentation2,\n",
    "    image_weight=0.7,\n",
    "    n_categories=num_concepts)\n",
    "\n",
    "visualization = visualization[:orig_shape[0], :orig_shape[1], :]\n",
    "visualization2 = visualization2[:orig_shape2[0], :orig_shape2[1], :]\n",
    "\n",
    "display(Image.fromarray(visualization))\n",
    "display(Image.fromarray(visualization2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
